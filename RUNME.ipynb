{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaperMill - Data Preprocessing\n",
    "This notebook contains all the necessary scripts used to clean the data used in PaperMill.\n",
    "Initially, the data is broken into smaller chunks for easier manipulation during analysis. During this process, the dataset is also divided into two subsets: Relationship and Detail (RD) and Textual Content.\n",
    "The RD data is then recombined for use in Neo4j.\n",
    "The Textual Content is cleaned for use in the recommender system. It is divided into a list of the paper IDs and text data and then recombined.\n",
    "Finally, the text data is used to generate a similarity matrix, the basis for the recommender system.\n",
    "<br>\n",
    "<br>\n",
    "To begin, the DBLP dataset can be downloaded [here](https://lfs.aminer.cn/misc/dblp.v11.zip)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Splitting the Dataset\n",
    "To begin, the dataset is split into the RD dataset and the Textual Content dataset. The RD data is converted to CSV format, while the Textual Content remains as JSON.\n",
    "The \"split_dataset\" function takes four parameters:\n",
    "1. file (string): Directory location of the dataset\n",
    "2. numRows (integer): Number of rows within the dataset\n",
    "3. splits (integer): Number of times to split the dataset\n",
    "4. output (string): Directroy location of the output files\n",
    "5. badIDs (list of strings): List of the corrupted entries identified in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-17 16:16:53.020776: Splitting initiated.\n",
      "Bad ID caught: 133370419.\n",
      "Bad ID caught: 1978543128.\n",
      "Bad ID caught: 1995561360.\n",
      "2020-05-17 16:32:27.918113: Split 1 complete.\n",
      "Bad ID caught: 2015718743.\n",
      "2020-05-17 16:49:06.626307: Split 2 complete.\n",
      "Bad ID caught: 2114368344.\n",
      "Bad ID caught: 2124878095.\n",
      "Bad ID caught: 2143245624.\n",
      "Bad ID caught: 2171998353.\n",
      "Bad ID caught: 2344857557.\n",
      "2020-05-17 17:06:03.531514: Split 3 complete.\n",
      "Bad ID caught: 2405346676.\n",
      "2020-05-17 17:22:02.218399: Split 4 complete.\n",
      "2020-05-17 17:22:02.218399: Datset split.\n"
     ]
    }
   ],
   "source": [
    "from utils.split import split_dataset\n",
    "\n",
    "fileName = \"dataset/DBLP_V11.json\"\n",
    "outputDir = \"dataset/split/\"\n",
    "badIDs = [\"133370419\",\"1978543128\",\"1995561360\",\"2015718743\",\"2114368344\",\"2124878095\",\"2143245624\",\"2171998353\",\"2344857557\",\"2405346676\"]\n",
    "\n",
    "split_dataset(fileName, 4107340, 4, outputDir, badIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the RD Dataset\n",
    "The RD dataset is then recombined using the \"merge_csvs\" function. This function takes six parameters:\n",
    "1. inputDir (string): Location of the split data\n",
    "2. outputDir (string): Location for the output file\n",
    "3. prefix (string): Prefix which points towards the correct data type\n",
    "4. outputFileName (string): Name of the outputted file\n",
    "5. splits (integer): Number of files the input data has been split into\n",
    "6. headers (boolean): Whether the split files have headers or not (default is True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-17 17:28:02.726589: Merge initiated.\n",
      "2020-05-17 17:28:15.387427: dataset/split/Split_DBLP_RD_1.csv merged.\n",
      "2020-05-17 17:28:21.921405: dataset/split/Split_DBLP_RD_2.csv merged.\n",
      "2020-05-17 17:28:28.455670: dataset/split/Split_DBLP_RD_3.csv merged.\n",
      "2020-05-17 17:28:34.210036: dataset/split/Split_DBLP_RD_4.csv merged.\n",
      "2020-05-17 17:28:34.211029: DBLP_RD.csv successfully merged.\n"
     ]
    }
   ],
   "source": [
    "from utils.merge import merge_csvs\n",
    "\n",
    "inputDir = \"dataset/split/\"\n",
    "outputDir = \"dataset/neo4j/\"\n",
    "prefix = \"Split_DBLP_RD_\"\n",
    "outputFileName = \"DBLP_RD\"\n",
    "\n",
    "merge_csvs(inputDir, outputDir, prefix, outputFileName, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Formatting the Textual Content\n",
    "The text data needs to be cleaned for use in the recommender system. This is carried out by the \"text_preprocessing\" function, which takes three parameters:\n",
    "1. inputDir (string): Location of the split textual content dataset\n",
    "2. outputDir (string): Location for the cleaned file\n",
    "3. splits (integer): Number of times the original DBLP dataset was split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.recommender import text_preprocessing\n",
    "\n",
    "inputDir = \"dataset/split/\"\n",
    "outputDir = \"dataset/recommender/\"\n",
    "\n",
    "text_preprocessing(inputDir, outputDir, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recommender System Framework\n",
    "The final step is to create the framework for the recommender system. This process is carried out when the \"generate_recommender\" function is called, which takes seven parameters.\n",
    "1. textFile (string): Location of the specified text file used in dictionary generation\n",
    "2. outputDir (string): Location to save recommender parts\n",
    "3. weighting (string): SMART-IRS TF-IDF weighting, default is ntc\n",
    "4. dictExists (boolean): Whether the Gensim dictionary exists already, default is False\n",
    "5. corpusExists (boolean): Whether the Gensim corpus exists already, default is False\n",
    "6. tfidfExists (boolean): Whether the Gensim TF-IDF model exists already, default is False\n",
    "7. createSimMatrix (boolean): Whether to create a disk based version of the similarity matrix, default is true\n",
    "\n",
    "For more information on the createSimMatrix parameter, please refer to the README file of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ronan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'datetime.datetime' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-abfe1ca6f6c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutputDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"src/static/data/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgenerate_recommender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweighting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ntc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictExists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpusExists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidfExists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\RÃ³nan\\Documents\\UCD\\Stage 4\\FYP\\Final\\utils\\recommender.py\u001b[0m in \u001b[0;36mgenerate_recommender\u001b[1;34m(textFile, outputDir, weighting, dictExists, corpusExists, tfidfExists, createSimMatrix)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": Recommender generation initiated.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdictExists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'datetime.datetime' and 'str'"
     ]
    }
   ],
   "source": [
    "from utils.recommender import generate_recommender\n",
    "\n",
    "textFile = \"dataset/recommender/Text.csv\"\n",
    "outputDir = \"src/static/data/\"\n",
    "\n",
    "generate_recommender(textFile, outputDir, weighting=\"ntc\", dictExists=False, corpusExists=False, \n",
    "                     tfidfExists=False, createSimMatrix=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
